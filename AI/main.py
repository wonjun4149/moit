# main_V3.py (main_tea.py ê¸°ë°˜ + StateGraph ì·¨ë¯¸ ì¶”ì²œ ì—ì´ì „íŠ¸ ì´ì‹)

# --- 1. ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ import ---
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import os
from dotenv import load_dotenv
import requests
import json
from datetime import datetime # ì˜¤ëŠ˜ ë‚ ì§œ í™•ì¸ì„ ìœ„í•´ ì¶”ê°€
from typing import List, TypedDict, Optional
import logging
from fastapi.middleware.cors import CORSMiddleware

# --- 2. ë¡œê¹… ê¸°ë³¸ ì„¤ì • ---
logging.basicConfig(level=logging.INFO, format='%(levelname)s:     %(message)s')

# --- 3. LangChain, LangGraph ë° AI ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ import ---
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import chain
from langchain_pinecone import PineconeVectorStore # ReAct Agent
from langgraph.prebuilt import create_react_agent
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langgraph.graph import StateGraph, END
from langchain_core.tools import tool
from tenacity import retry, stop_after_attempt, wait_fixed
import google.generativeai as genai # Gemini ì¶”ê°€
from requests.exceptions import Timeout, ConnectionError
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain.tools.retriever import create_retriever_tool
from langchain_core.documents import Document

# --- 4. í™˜ê²½ ì„¤ì • ë° FastAPI ì•± ì´ˆê¸°í™” ---
load_dotenv()
app = FastAPI(
    title="MOIT AI Agent Server v3 (StateGraph ì´ì‹)",
    description="main_tea.py ê¸°ë°˜ ìœ„ì— StateGraph ì·¨ë¯¸ ì¶”ì²œ ì—ì´ì „íŠ¸ë¥¼ ì´ì‹í•œ ë²„ì „",
    version="3.0.0",
)

# --- CORS ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€ ---
origins = ["*"]
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- AI ëª¨ë¸ ë° API í‚¤ ì„¤ì • ---
try:
    gemini_api_key = os.getenv("GOOGLE_API_KEY")
    if gemini_api_key:
        genai.configure(api_key=gemini_api_key)
    else:
        logging.warning("GOOGLE_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‚¬ì§„ ë¶„ì„ ê¸°ëŠ¥ì´ ì‘ë™í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
except Exception as e:
    logging.warning(f"Gemini API í‚¤ ì„¤ì • ì‹¤íŒ¨: {e}")

llm = ChatOpenAI(model="gpt-4o-mini")


# --- 5. ë§ˆìŠ¤í„° ì—ì´ì „íŠ¸ ë¡œì§ ì „ì²´ ì •ì˜ ---

# 5-1. ë§ˆìŠ¤í„° ì—ì´ì „íŠ¸ì˜ State ì •ì˜
class MasterAgentState(TypedDict):
    user_input: dict
    route: str
    final_answer: str

# 5-2. ë¼ìš°í„° ë…¸ë“œ ì •ì˜
router_prompt = ChatPromptTemplate.from_template(
    """ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ì–´ë–¤ ë‹´ë‹¹ìì—ê²Œ ì „ë‹¬í•´ì•¼ í• ì§€ ê²°ì •í•˜ëŠ” AI ë¼ìš°í„°ì…ë‹ˆë‹¤.
    ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë³´ê³ , ì•„ë˜ ì„¸ ê°€ì§€ ê²½ë¡œ ì¤‘ ê°€ì¥ ì ì ˆí•œ ê²½ë¡œ í•˜ë‚˜ë§Œ ê³¨ë¼ ê·¸ ì´ë¦„ë§Œ ì •í™•íˆ ë‹µë³€í•´ì£¼ì„¸ìš”.
 
    [ê²½ë¡œ ì„¤ëª…]
    - `meeting_matching`: ì‚¬ìš©ìê°€ 'ìƒˆë¡œìš´ ëª¨ì„'ì„ ë§Œë“¤ë ¤ê³  í•  ë•Œ, ê¸°ì¡´ì— ìˆë˜ 'ìœ ì‚¬í•œ ëª¨ì„'ì„ ì¶”ì²œí•´ì£¼ëŠ” ê²½ë¡œì…ë‹ˆë‹¤. ì…ë ¥ì— 'title', 'description' í‚¤ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ì´ ê²½ë¡œì¼ í™•ë¥ ì´ ë†’ìŠµë‹ˆë‹¤.
    - `hobby_recommendation`: ì‚¬ìš©ìì—ê²Œ 'ìƒˆë¡œìš´ ì·¨ë¯¸' ìì²´ë¥¼ ì¶”ì²œí•´ì£¼ëŠ” ê²½ë¡œì…ë‹ˆë‹¤. ì…ë ¥ì— 'survey' í‚¤ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ì´ ê²½ë¡œì¼ í™•ë¥ ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤.
    - `general_search`: ì‚¬ìš©ìê°€ ë‚ ì”¨, ë§›ì§‘, íŠ¹ì • ì •ë³´ ê²€ìƒ‰ ë“± 'ì¼ë°˜ì ì¸ ì§ˆë¬¸'ì„ í•˜ê±°ë‚˜, 'ëª¨ì„/ì·¨ë¯¸ ì¶”ì²œ' ì´ì™¸ì˜ ëŒ€í™”ë¥¼ ì‹œë„í•  ë•Œ ì‚¬ìš©ë˜ëŠ” ê²½ë¡œì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ "ì£¼ë§ì— ë¹„ ì˜¤ëŠ”ë° ë­í•˜ì§€?" ì™€ ê°™ì€ ì§ˆë¬¸ì´ í•´ë‹¹ë©ë‹ˆë‹¤.
 
    [ì‚¬ìš©ì ìš”ì²­]:
    {user_input}
 
    [íŒë‹¨ ê²°ê³¼ (meeting_matching, hobby_recommendation, ë˜ëŠ” general_search)]:
    """
)
router_chain = router_prompt | llm | StrOutputParser()

def route_request(state: MasterAgentState):
    """ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë³´ê³  ì–´ë–¤ ì „ë¬¸ê°€ì—ê²Œ ë³´ë‚¼ì§€ ê²°ì •í•˜ëŠ” ë…¸ë“œ"""
    logging.info("--- ROUTING ---")
    # [ìˆ˜ì •] ë¶ˆí•„ìš”í•œ ë¡œê·¸ ë¼ì¸ì„ ì œê±°í•˜ê³ , invokeì— user_inputì„ ì§ì ‘ ì „ë‹¬í•©ë‹ˆë‹¤.
    route_decision = router_chain.invoke({"user_input": state['user_input']})
    cleaned_decision = route_decision.strip().lower().replace("'", "").replace('"', '')
    logging.info(f"ë¼ìš°íŒ… ê²°ì •: {cleaned_decision}")
    return {"route": cleaned_decision}

# 5-3. ì „ë¬¸ê°€ í˜¸ì¶œ ë…¸ë“œë“¤ ì •ì˜

# --- ë²”ìš© ê²€ìƒ‰ ì—ì´ì „íŠ¸ì—ì„œ ì‚¬ìš©í•  ë„êµ¬ ---
@tool
def get_current_date() -> str:
    """ì˜¤ëŠ˜ ë‚ ì§œë¥¼ 'YYYYë…„ MMì›” DDì¼' í˜•ì‹ì˜ ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤. 'ì˜¤ëŠ˜', 'ë‚´ì¼', 'ì´ë²ˆ ì£¼'ì™€ ê°™ì€ ìƒëŒ€ì ì¸ ì‹œê°„ í‘œí˜„ì„ ì •í™•íˆ í•´ì„í•´ì•¼ í•  ë•Œ ì‚¬ìš©í•˜ì„¸ìš”."""
    return datetime.now().strftime("%Yë…„ %mì›” %dì¼")


# ì „ë¬¸ê°€ 0: ë²”ìš© ê²€ìƒ‰ ì—ì´ì „íŠ¸ (ì‹ ê·œ ì¶”ê°€)
def call_general_search_agent(state: MasterAgentState):
    """'ë²”ìš© ê²€ìƒ‰ ì—ì´ì „íŠ¸'ë¥¼ í˜¸ì¶œí•˜ì—¬ ì›¹ ê²€ìƒ‰ ë˜ëŠ” ë‚´ë¶€ ëª¨ì„ DB ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” ë…¸ë“œ"""
    logging.info("--- CALLING: General Search Agent ---")

    # 1. ë„êµ¬ ì •ì˜
    # 1-1. ì›¹ ê²€ìƒ‰ ë„êµ¬
    tavily_tool = TavilySearchResults(max_results=3, name="web_search")
    tavily_tool.description = "ë‚ ì”¨, ë‰´ìŠ¤, ë§›ì§‘, íŠ¹ì • ì£¼ì œì— ëŒ€í•œ ìµœì‹  ì •ë³´ ë“± ì™¸ë¶€ ì„¸ê³„ì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µí•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤."

    @retry(stop=stop_after_attempt(3), wait=wait_fixed(2))
    def web_search_with_retry(query: str):
        """ë‚ ì”¨, ë‰´ìŠ¤, ë§›ì§‘, íŠ¹ì • ì£¼ì œì— ëŒ€í•œ ìµœì‹  ì •ë³´ ë“± ì™¸ë¶€ ì„¸ê³„ì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µí•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤."""
        try:
            return tavily_tool.invoke({"query": query})
        except (Timeout, ConnectionError) as e:
            logging.error(f"Web search failed due to connection error or timeout: {e}")
            raise  # ë‹¤ì‹œ ì‹œë„í•˜ë ¤ë©´ ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œí‚µë‹ˆë‹¤.
        except Exception as e:
            logging.error(f"Web search failed with error: {e}")
            return f"ì˜¤ë¥˜: ì›¹ ê²€ìƒ‰ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. {e}"  # ì˜¤ë¥˜ ë©”ì‹œì§€ ë°˜í™˜
        except Exception as e:
            logging.error(f"Web search failed with error: {e}")
            raise  # Re-raise the exception to trigger retry


    web_search_with_retry.description = "ë‚ ì”¨, ë‰´ìŠ¤, ë§›ì§‘, íŠ¹ì • ì£¼ì œì— ëŒ€í•œ ìµœì‹  ì •ë³´ ë“± ì™¸ë¶€ ì„¸ê³„ì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µí•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤."
    # 1-2. ë‚´ë¶€ ëª¨ì„ DB ê²€ìƒ‰ ë„êµ¬ (ê¸°ì¡´ ë¡œì§ ì¬í™œìš©)
    meeting_index_name = os.getenv("PINECONE_INDEX_NAME_MEETING")
    if not meeting_index_name: raise ValueError("'.env' íŒŒì¼ì— PINECONE_INDEX_NAME_MEETING ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
    embedding_function = OpenAIEmbeddings(model='text-embedding-3-large')
    vector_store = PineconeVectorStore.from_existing_index(index_name=meeting_index_name, embedding=embedding_function)
    retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={'k': 3})
    
    moit_meeting_retriever_tool = create_retriever_tool(
        retriever,
        "moit_internal_meeting_search",
        "MOIT ì„œë¹„ìŠ¤ ë‚´ì— ë“±ë¡ëœ ê¸°ì¡´ ëª¨ì„ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤. 'ì‹¤ë‚´ í™œë™', 'ì„œìš¸ ì§€ì—­ ì£¼ë§ ëª¨ì„' ë“± ì‚¬ìš©ìê°€ ì°¾ëŠ” ì¡°ê±´ì— ë§ëŠ” ëª¨ì„ì„ ì°¾ì•„ ì¶”ì²œí•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤."
    )

    # 1-3. ì˜¤ëŠ˜ ë‚ ì§œ í™•ì¸ ë„êµ¬
    get_current_date_tool = get_current_date
    tools = [web_search_with_retry, moit_meeting_retriever_tool]

    tools = [tavily_tool, moit_meeting_retriever_tool, get_current_date_tool]

    # 2. ReAct ì—ì´ì „íŠ¸ ìƒì„±
    # [ì¤‘ìš”] ReAct í”„ë¡¬í”„íŠ¸ì— ì—ì´ì „íŠ¸ì˜ ì—­í• ê³¼ ë„êµ¬ ì‚¬ìš©ë²•ì„ ëª…í™•íˆ ì§€ì‹œí•©ë‹ˆë‹¤.
    react_prompt = ChatPromptTemplate.from_messages(

        [
            ("system", """ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ê°€ì¥ ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ 'MOIT'ì…ë‹ˆë‹¤.
            ë‹¹ì‹ ì€ ì„¸ ê°€ì§€ ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤: 'web_search', 'moit_internal_meeting_search', 'get_current_date'.

            [ì§€ì¹¨]
            1. **ì‹œê°„ ì¸ì‹**: ì§ˆë¬¸ì— 'ì˜¤ëŠ˜', 'ë‚´ì¼' ë“± ìƒëŒ€ì  ì‹œê°„ì´ ìˆìœ¼ë©´, ë¨¼ì € `get_current_date`ë¡œ ì˜¤ëŠ˜ ë‚ ì§œë¥¼ í™•ì¸í•˜ê³ , ê·¸ ë‹¤ìŒ `web_search`ë¡œ ê°€ì¥ ê³µì‹ ë ¥ìˆëŠ” ì •ë³´ë¥¼ ì°¾ìœ¼ì„¸ìš”. (ì˜ˆ: ë‚ ì”¨ëŠ” ë„¤ì´ë²„ë‚ ì”¨(weather.naver.com)ë¥¼ í†µí•´ ê²€ìƒ‰í•˜ì„¸ìš”)
            2. **ëª¨ì„ ê²€ìƒ‰**: "ì¶•êµ¬ í•˜ê³  ì‹¶ì€ë° ëª¨ì„ ì—†ë‚˜?"ì²˜ëŸ¼ MOIT ì„œë¹„ìŠ¤ ë‚´ ëª¨ì„ì„ ì°¾ëŠ” ìš”ì²­ì—ëŠ” `moit_internal_meeting_search`ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
            3. **ë³µí•© ì§ˆë¬¸ ì²˜ë¦¬**: "ë¹„ ì˜¤ëŠ” ì£¼ë§ì— ?" ê°™ì€ ì§ˆë¬¸ì—ëŠ”, `web_search`ë¡œ ë‚ ì”¨ë¥¼ í™•ì¸í•œ í›„, ê·¸ ê²°ê³¼ë¥¼ ì´ìš©í•´ `moit_internal_meeting_search`ë¡œ 'ì‹¤ë‚´ ëª¨ì„'ì„ ì°¾ê±°ë‚˜ ì—†ë‹¤ë©´ ìƒˆë¡œìš´ ì·¨ë¯¸ë¥¼ ì¶”ì²œí•´ì£¼ëŠ” ë“± ë„êµ¬ë¥¼ ì¡°í•©í•˜ì—¬ ìµœì ì˜ ë‹µì„ ì°¾ìœ¼ì„¸ìš”.
            4. **ì¼ë°˜ ì§ˆë¬¸**: ê·¸ ì™¸ ëª¨ë“  ì¼ë°˜ì ì¸ ì§ˆë¬¸(ë‰´ìŠ¤, ë§›ì§‘, ìƒì‹, ì¶”ì²œ)ì€ `web_search`ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
            
            ìµœì¢… ë‹µë³€ì€ í•­ìƒ ì¹œì ˆí•œ ë§íˆ¬ë¡œ ì •ë¦¬í•˜ê³ , ëª¨ì„ì„ ì¶”ì²œí•  ë•ŒëŠ” ì°¸ì—¬ë¥¼ ìœ ë„í•˜ëŠ” ë¬¸êµ¬ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.
            """),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"), # "user_input" ëŒ€ì‹  "input" ì‚¬ìš©
            MessagesPlaceholder(variable_name="agent_scratchpad"),
            MessagesPlaceholder(variable_name="chat_history", optional=True),
            ("human", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad")
        ]
    )
    
    general_agent_runnable = create_react_agent(llm, tools, react_prompt)
    agent = create_openai_tools_agent(llm, tools, react_prompt)
    general_agent_runnable = AgentExecutor(agent=agent, tools=tools, verbose=True)

    # 3. ì—ì´ì „íŠ¸ ì‹¤í–‰
    # MasterAgentì˜ user_input í˜•ì‹ì— ë§ê²Œ ì‹¤ì œ ì§ˆë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    # ì˜ˆ: {'messages': [['user', 'ì£¼ë§ì— ë¹„ì˜¤ëŠ”ë° ë­í•˜ì§€?']]}
    try:
        user_question = state['user_input']['messages'][0][1]
    except (KeyError, IndexError):
        # ë§Œì•½ ìœ„ êµ¬ì¡°ê°€ ì•„ë‹ ê²½ìš°, user_input ì „ì²´ë¥¼ ì‚¬ìš©
        user_question = str(state['user_input'])

    input_data = {"input": user_question, "chat_history": []} # chat_history ì¶”ê°€
    logging.info(f"ë²”ìš© ê²€ìƒ‰ ì—ì´ì „íŠ¸ì—ê²Œ ì „ë‹¬ëœ ì§ˆë¬¸: {user_question}") # ë¡œê¹… ìˆ˜ì •
    logging.info(f"general_agent_runnable.invokeì— ì „ë‹¬ë˜ëŠ” ì…ë ¥: {input_data}") # ë¡œê¹… ì¶”ê°€

    result = general_agent_runnable.invoke(input_data) # input_dataë¥¼ ì‚¬ìš©í•˜ì—¬ í˜¸ì¶œ
    logging.info(f"ë²”ìš© ê²€ìƒ‰ ì—ì´ì „íŠ¸ì—ê²Œ ì „ë‹¬ëœ ì§ˆë¬¸: {user_question}")
    
    result = general_agent_runnable.invoke({"input": user_question})
    
    final_answer = result.get("output", "ì§ˆë¬¸ì„ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”.")
    logging.info(f"ë²”ìš© ê²€ìƒ‰ ì—ì´ì „íŠ¸ì˜ ìµœì¢… ë‹µë³€: {final_answer}")
    
    return {"final_answer": final_answer}

# ì „ë¬¸ê°€ 1: ëª¨ì„ ë§¤ì¹­ ì—ì´ì „íŠ¸ (SubGraph) - main_tea.py ì½”ë“œ ê¸°ë°˜
def call_meeting_matching_agent(state: MasterAgentState):
    """'ëª¨ì„ ë§¤ì¹­ ì—ì´ì „íŠ¸'ë¥¼ ë…ë¦½ì ì¸ SubGraphë¡œ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°›ì•„ì˜¤ëŠ” ë…¸ë“œ"""
    logging.info("--- CALLING: Meeting Matching Agent ---")

    class MeetingAgentState(TypedDict):
        title: str; description: str; time: str; location: str; query: str
        context: List[Document]; answer: str; decision: str; rewrite_count: int

    meeting_llm = ChatOpenAI(model="gpt-4o-mini")
    meeting_index_name = os.getenv("PINECONE_INDEX_NAME_MEETING")
    if not meeting_index_name: raise ValueError("'.env' íŒŒì¼ì— PINECONE_INDEX_NAME_MEETING ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
    
    embedding_function = OpenAIEmbeddings(model='text-embedding-3-large')
    vector_store = PineconeVectorStore.from_existing_index(index_name=meeting_index_name, embedding=embedding_function)
    retriever = vector_store.as_retriever(
        search_type="similarity_score_threshold",
        search_kwargs={'score_threshold': 0.75, 'k': 2}
    )  

    prepare_query_prompt = ChatPromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìœ ì‚¬í•œ ë‹¤ë¥¸ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê¸° ìœ„í•œ ìµœì ì˜ ê²€ìƒ‰ì–´ë¥¼ ë§Œë“œëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        ì•„ë˜ [ëª¨ì„ ì •ë³´]ë¥¼ ì¢…í•©í•˜ì—¬, ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìœ ì‚¬í•œ ëª¨ì„ì„ ì°¾ê¸° ìœ„í•œ ê°€ì¥ í•µì‹¬ì ì¸ ê²€ìƒ‰ ì§ˆë¬¸ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
        [ëª¨ì„ ì •ë³´]:
- ì œëª©: {title}
- ì„¤ëª…: {description}
- ì‹œê°„: {time}
- ì¥ì†Œ: {location}"""
    )
    prepare_query_chain = prepare_query_prompt | meeting_llm | StrOutputParser()
    def prepare_query(m_state: MeetingAgentState):
        logging.info("--- (Sub) Preparing Query ---")
        query = prepare_query_chain.invoke({"title": m_state['title'], "description": m_state['description'], "time": m_state.get('time', ''), "location": m_state.get('location', '')})
        logging.info(f"ìƒì„±ëœ ê²€ìƒ‰ì–´: {query}")
        return {"query": query}

    def retrieve(m_state: MeetingAgentState):
        logging.info("--- (Sub) Retrieving Context from DB ---")
        context = retriever.invoke(m_state["query"])
        logging.info(f"DBì—ì„œ {len(context)}ê°œì˜ ìœ ì‚¬ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        return {"context": context}

    generate_prompt = ChatPromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë§¤ìš° ì—„ê²©í•˜ê²Œ ë¶„ì„í•˜ì—¬ ìœ ì‚¬í•œ ëª¨ì„ì„ ì¶”ì²œí•˜ëŠ” MOIT í”Œë«í¼ì˜ AIì…ë‹ˆë‹¤.
        ì‚¬ìš©ìê°€ ë§Œë“¤ë ¤ëŠ” ëª¨ì„ê³¼ **ì£¼ì œ, í™œë™ ë‚´ìš©ì´ ëª…í™•í•˜ê²Œ ì¼ì¹˜í•˜ëŠ”** ê¸°ì¡´ ëª¨ì„ë§Œ ì¶”ì²œí•´ì•¼ í•©ë‹ˆë‹¤.

        [ì‚¬ìš©ì ì…ë ¥ ì •ë³´]:
        {query}

        [ê²€ìƒ‰ëœ ìœ ì‚¬ ëª¨ì„ ì •ë³´]:
        {context}

        [ì§€ì‹œì‚¬í•­]:
        1. [ê²€ìƒ‰ëœ ìœ ì‚¬ ëª¨ì„ ì •ë³´]ì˜ ê° í•­ëª©ì„ [ì‚¬ìš©ì ì…ë ¥ ì •ë³´]ì™€ ë¹„êµí•˜ì—¬, **ì •ë§ë¡œ ê´€ë ¨ì„±ì´ ë†’ë‹¤ê³  íŒë‹¨ë˜ëŠ” ëª¨ì„ë§Œ** ê³¨ë¼ëƒ…ë‹ˆë‹¤. (ì˜ˆ: 'ì¶•êµ¬' ëª¨ì„ì„ ì°¾ëŠ” ì‚¬ìš©ìì—ê²Œ 'ì•¼êµ¬' ëª¨ì„ì€ ì¶”ì²œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.)
        2. 1ë²ˆì—ì„œ ê³¨ë¼ë‚¸ ëª¨ì„ì´ ìˆë‹¤ë©´, í•´ë‹¹ ëª¨ì„ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ì œì•ˆí•  ì¶”ì²œì‚¬ë¥¼ ì¹œì ˆí•œ ë§íˆ¬("~ëŠ” ì–´ë– ì„¸ìš”?")ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
        3. 1ë²ˆì—ì„œ ê³¨ë¼ë‚¸ ëª¨ì„ì˜ `meeting_id`ì™€ `title`ì„ ì¶”ì¶œí•˜ì—¬ `recommendations` ë°°ì—´ì„ êµ¬ì„±í•©ë‹ˆë‹¤. **ì¶”ì²œí•  ëª¨ì„ì´ í•˜ë‚˜ë¿ì´ë¼ë©´, ë°°ì—´ì— í•˜ë‚˜ë§Œ í¬í•¨í•©ë‹ˆë‹¤.**
        4. ìµœì¢… ë‹µë³€ì„ ì•„ë˜ì™€ ê°™ì€ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì œê³µí•´ì£¼ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

        [JSON ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ]:
        // ì¶”ì²œí•  ëª¨ì„ì´ 1ê°œì¼ ê²½ìš°
        {{
            "summary": "ë¹„ìŠ·í•œ ì¶•êµ¬ ëª¨ì„ì´ ìˆëŠ”ë°, ì°¸ì—¬í•´ ë³´ì‹œëŠ” ê±´ ì–´ë– ì„¸ìš”?",
            "recommendations": [
                {{ "meeting_id": "ì¶•êµ¬ ëª¨ì„ ID", "title": "ê°™ì´ ì¶•êµ¬í•˜ì‹¤ ë¶„!" }}
            ]
        }}

        // 1ë²ˆì—ì„œ ê³¨ë¼ë‚¸ ëª¨ì„ì´ ì—†ì„ ê²½ìš° (ì¶”ì²œí•  ë§Œí•œ ëª¨ì„ì´ ì—†ëŠ” ê²½ìš°)
        {{
            "summary": "",
            "recommendations": []
        }}
        """
    )
    generate_chain = generate_prompt | meeting_llm | StrOutputParser()
    def generate(m_state: MeetingAgentState):
        logging.info("--- (Sub) Generating Final Answer ---")
        context_str = ""
        for i, doc in enumerate(m_state['context']):
            metadata = doc.metadata or {}
            meeting_id = metadata.get('meeting_id', 'N/A')
            title = metadata.get('title', 'N/A')
            context_str += f"ëª¨ì„ {i+1}:\n  - meeting_id: {meeting_id}\n  - title: {title}\n  - content: {doc.page_content}\n\n"
        if not m_state['context']: context_str = "ìœ ì‚¬í•œ ëª¨ì„ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        logging.info(f"--- (Sub) ìµœì¢… ì¶”ì²œ ìƒì„±ì„ ìœ„í•´ LLMì— ì „ë‹¬í•  ì»¨í…ìŠ¤íŠ¸ ---\n{context_str}")
        answer = generate_chain.invoke({"context": context_str, "query": m_state['query']})
        return {"answer": answer}

    check_helpfulness_prompt = ChatPromptTemplate.from_template(
        """ë‹¹ì‹ ì€ AI ë‹µë³€ì„ í‰ê°€í•˜ëŠ” ì—„ê²©í•œ í‰ê°€ê´€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ [AI ë‹µë³€]ì´ ì‚¬ìš©ìì˜ [ì›ë³¸ ì§ˆë¬¸] ì˜ë„ì— ëŒ€í•´ ìœ ìš©í•œ ì œì•ˆì„ í•˜ëŠ”ì§€ í‰ê°€í•´ì£¼ì„¸ìš”.
        ë‹¤ë¥¸ ì„¤ëª…ì€ ì¼ì ˆ ì¶”ê°€í•˜ì§€ ë§ê³ , ì˜¤ì§ 'helpful' ë˜ëŠ” 'unhelpful' ë‘˜ ì¤‘ í•˜ë‚˜ì˜ ë‹¨ì–´ë¡œë§Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.

        [ì›ë³¸ ì§ˆë¬¸]: {query}
        [AI ë‹µë³€]: {answer}
        
        [í‰ê°€ ê²°ê³¼ (helpful ë˜ëŠ” unhelpful)]:"""
    )
    check_helpfulness_chain = check_helpfulness_prompt | meeting_llm | StrOutputParser()
    def check_helpfulness(m_state: MeetingAgentState):
        logging.info("--- (Sub) Checking Helpfulness ---")
        raw_result = check_helpfulness_chain.invoke({"query": m_state['query'], "answer": m_state['answer']})
        
        cleaned_result = raw_result.strip().lower().replace('"', '').replace("'", "")
        decision = "helpful" if cleaned_result == "helpful" else "unhelpful"
        
        logging.info(f"ë‹µë³€ ìœ ìš©ì„± í‰ê°€ (Raw): {raw_result} -> (Parsed): {decision}")
        return {"decision": decision}

    rewrite_query_prompt = ChatPromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ë” ë‚˜ì€ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìœ„í•´ ì§ˆë¬¸ì„ ì¬êµ¬ì„±í•˜ëŠ” í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤.
        [ì›ë³¸ ì§ˆë¬¸]ì€ ë²¡í„° ê²€ìƒ‰ì—ì„œ ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì›ë³¸ ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ëŠ” ìœ ì§€í•˜ë˜, ì™„ì „íˆ ë‹¤ë¥¸ ê´€ì ì—ì„œ ì ‘ê·¼í•˜ê±°ë‚˜, ë” êµ¬ì²´ì ì¸ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ì„± ë†’ì€ ëª¨ì„ì„ ì°¾ì„ ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ ê²€ìƒ‰ ì§ˆë¬¸ì„ í•˜ë‚˜ë§Œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
        [ì›ë³¸ ì§ˆë¬¸]: {query}
        [ìƒˆë¡œìš´ ê²€ìƒ‰ ì§ˆë¬¸]:"""
    )
    rewrite_query_chain = rewrite_query_prompt | meeting_llm | StrOutputParser()
    def rewrite_query(m_state: MeetingAgentState):
        logging.info("--- (Sub) Rewriting Query ---")
        new_query = rewrite_query_chain.invoke({"query": m_state['query']})
        logging.info(f"ì¬ì‘ì„±ëœ ê²€ìƒ‰ì–´: {new_query}")
        count = m_state.get('rewrite_count', 0) + 1
        return {"query": new_query, "rewrite_count": count}

    def decide_to_continue(state: MeetingAgentState):
        if state.get("rewrite_count", 0) >= 2: 
            logging.info(f"--- ì¬ì‹œë„ íšŸìˆ˜({state.get('rewrite_count', 0)}) ì´ˆê³¼í–ˆê¸°ì— ë£¨í”„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. ---")
            return END
        if state.get("decision") == "helpful":
            return END
        return "rewrite_query"

    graph_builder = StateGraph(MeetingAgentState)
    graph_builder.add_node("prepare_query", prepare_query)
    graph_builder.add_node("retrieve", retrieve)
    graph_builder.add_node("generate", generate)
    graph_builder.add_node("check_helpfulness", check_helpfulness)
    graph_builder.add_node("rewrite_query", rewrite_query)
    graph_builder.set_entry_point("prepare_query")
    graph_builder.add_edge("prepare_query", "retrieve")
    graph_builder.add_edge("retrieve", "generate")
    graph_builder.add_edge("generate", "check_helpfulness")
    graph_builder.add_conditional_edges("check_helpfulness", decide_to_continue)
    graph_builder.add_edge("rewrite_query", "retrieve")
    meeting_agent = graph_builder.compile()

    # create_meeting.phpì—ì„œ ì˜¤ëŠ” ë°ì´í„° í˜•ì‹ì— ë§ê²Œ ì‹¤ì œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    user_input = state['user_input'].get('messages', [[]])[0][1] if 'messages' in state['user_input'] else state['user_input']
    initial_state = {
        "title": user_input.get("title", ""),
        "description": user_input.get("description", ""),
        "time": user_input.get("time", ""),
        "location": user_input.get("location", ""),
        "rewrite_count": 0
    }
    
    final_result_state = meeting_agent.invoke(initial_state, {"recursion_limit": 15})

    final_decision = final_result_state.get("decision")
    final_answer = final_result_state.get("answer")

    if final_decision == 'helpful':
        logging.info("--- ìµœì¢… ê²°ì •ì´ 'helpful'ì´ë¯€ë¡œ, ìƒì„±ëœ ì¶”ì²œì•ˆì„ ë°˜í™˜í•©ë‹ˆë‹¤. ---")
        return {"final_answer": final_answer}
    else:
        logging.info("--- ìµœì¢… ê²°ì •ì´ 'helpful'ì´ ì•„ë‹ˆë¯€ë¡œ, ì‹ ê·œ ìƒì„±ì„ ìœ ë„í•˜ê¸° ìœ„í•´ ë¹ˆ ì¶”ì²œì•ˆì„ ë°˜í™˜í•©ë‹ˆë‹¤. ---")
        empty_recommendation = json.dumps({"summary": "", "recommendations": []})
        return {"final_answer": empty_recommendation}


# --- ì „ë¬¸ê°€ 2: ì·¨ë¯¸ ì¶”ì²œ ì—ì´ì „íŠ¸ (StateGraph ê¸°ë°˜ìœ¼ë¡œ êµì²´) ---

# [ìˆ˜ì •] ì·¨ë¯¸ ì¶”ì²œ ì—ì´ì „íŠ¸ê°€ ì‚¬ìš©í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
def normalize(value, min_val, max_val):
    """(ê¸°ì¡´ _normalizeì—ì„œ ì´ë¦„ ë³€ê²½)"""
    if value is None: return None
    return round((value - min_val) / (max_val - min_val), 4)

def generate_prompt(profile):
    """[ì‹ ê·œ ì¶”ê°€] ì‚¬ìš©ì í”„ë¡œí•„ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°›ì•„ Geminiì—ê²Œ ë³´ë‚¼ ìì—°ì–´ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    
    def get_val(val, format_str="{:.2f}"):
        if val is None: return "N/A"
        if format_str: return format_str.format(val)
        return val

    # í”„ë¡œí•„ì˜ ê° ì„¹ì…˜ì„ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    fsc = profile.get('FSC', {})
    pssr = profile.get('PSSR', {})
    mp = profile.get('MP', {})
    dls = profile.get('DLS', {})
    ip = profile.get('IP', {}) # â˜… ê´€ì‹¬ì‚¬ í”„ë¡œí•„(IP)ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.

    # --- í”„ë¡œí•„ ìš”ì•½ ---
    fsc_summary = (f"* **í˜„ì‹¤ì  ì œì•½**: ì‹œê°„({get_val(fsc.get('time_availability'))}), ì˜ˆì‚°({get_val(fsc.get('financial_budget'))}), ì—ë„ˆì§€({get_val(fsc.get('energy_level'))}), ì´ë™ì„±({get_val(fsc.get('mobility'))}) / ì„ í˜¸ê³µê°„: {get_val(fsc.get('preferred_space'), format_str=None)}")
    pssr_summary = (f"* **ì‹¬ë¦¬ì  ìƒíƒœ**: ì‚¬íšŒì  ë¶ˆì•ˆ({get_val(pssr.get('social_anxiety_score'))}), í˜„ì¬ ê³ ë¦½ ìˆ˜ì¤€({get_val(pssr.get('isolation_level'))}) (0:ê³ ë¦½, 1:í™œë°œ)")
    mp_summary = (f"* **ì£¼ìš” ë™ê¸°**: í•µì‹¬ ëª©í‘œëŠ” '{get_val(mp.get('core_motivation'), format_str=None)}' ì…ë‹ˆë‹¤.")
    dls_summary = (f"* **ì‚¬íšŒì„± ì„ í˜¸**: '{get_val(dls.get('preferred_sociality_type'), format_str=None)}' í™œë™ì„ ì„ í˜¸í•©ë‹ˆë‹¤.")
    
    # --- â˜…â˜…â˜… ì´ ë¶€ë¶„ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤ â˜…â˜…â˜… ---
    # ê´€ì‹¬ì‚¬ í”„ë¡œí•„ ìš”ì•½ ìƒì„±
    ip_summary = (f"* **ê´€ì‹¬ì‚¬ í”„ë¡œí•„** (0:ê´€ì‹¬ì—†ìŒ, 1:ë§¤ìš°ê´€ì‹¬): ìì—°({get_val(ip.get('nature_interest'))}), "
                  f"ì†ìœ¼ë¡œ ë§Œë“¤ê¸°({get_val(ip.get('craft_interest'))}), "
                  f"ì§€ì  íƒêµ¬({get_val(ip.get('intellect_interest'))}), "
                  f"ì˜ˆìˆ ({get_val(ip.get('art_interest'))}), "
                  f"ì‹ ì²´ í™œë™({get_val(ip.get('activity_interest'))})")

    # ê°•ë ¥í•œ ê¸ˆì§€ ê·œì¹™ ìƒì„±
    hard_constraints = "\n# â˜…â˜…â˜… ê¸ˆì§€ ê·œì¹™ (Hard Constraints) â˜…â˜…â˜…\n"
    hard_constraints += "ì•„ë˜ ê·œì¹™ì€ ì‚¬ìš©ìì˜ ëª…ì‹œì ì¸ ì˜ì‚¬ì´ë¯€ë¡œ, ì‚¬ì§„ì˜ ë‚´ìš©ê³¼ ìƒì¶©ë˜ë”ë¼ë„ ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•©ë‹ˆë‹¤:\n"
    
    if ip.get('nature_interest', 0.5) < 0.3: # 0.5ëŠ” ê¸°ë³¸ê°’, 0.3ì€ 'ê´€ì‹¬ ì—†ìŒ' ê¸°ì¤€
        hard_constraints += "- **ê¸ˆì§€**: 'ìì—°' ê´€ë ¨ í™œë™ (ì˜ˆ: í…ƒë°­ ê°€ê¾¸ê¸°, ì‚°ì±…, ë“±ì‚°)ì€ ì‚¬ìš©ìì˜ ê´€ì‹¬ë„ê°€ ë§¤ìš° ë‚®ìœ¼ë¯€ë¡œ ì ˆëŒ€ ì¶”ì²œí•˜ì§€ ë§ˆì„¸ìš”.\n"
    if ip.get('craft_interest', 0.5) < 0.3:
        hard_constraints += "- **ê¸ˆì§€**: 'ì†ìœ¼ë¡œ ë§Œë“¤ê¸°' ê´€ë ¨ í™œë™ (ì˜ˆ: ê³µì˜ˆ, ìš”ë¦¬)ì€ ì ˆëŒ€ ì¶”ì²œí•˜ì§€ ë§ˆì„¸ìš”.\n"
    if ip.get('intellect_interest', 0.5) < 0.3:
        hard_constraints += "- **ê¸ˆì§€**: 'ì§€ì  íƒêµ¬' ê´€ë ¨ í™œë™ (ì˜ˆ: ë…ì„œ, ê³µë¶€)ì€ ì ˆëŒ€ ì¶”ì²œí•˜ì§€ ë§ˆì„¸ìš”.\n"
    if ip.get('art_interest', 0.5) < 0.3:
        hard_constraints += "- **ê¸ˆì§€**: 'ì˜ˆìˆ ' ê´€ë ¨ í™œë™ (ì˜ˆ: ê·¸ë¦¼, ìŒì•…)ì€ ì ˆëŒ€ ì¶”ì²œí•˜ì§€ ë§ˆì„¸ìš”.\n"
    if ip.get('activity_interest', 0.5) < 0.3:
        hard_constraints += "- **ê¸ˆì§€**: 'ì‹ ì²´ í™œë™' ê´€ë ¨ í™œë™ (ì˜ˆ: ìš´ë™, ì¶¤)ì€ ì ˆëŒ€ ì¶”ì²œí•˜ì§€ ë§ˆì„¸ìš”.\n"
    # --- â˜…â˜…â˜… ì—¬ê¸°ê¹Œì§€ â˜…â˜…â˜… ---


    # ìµœì¢… í”„ë¡¬í”„íŠ¸ ì¡°í•©
    prompt = f"""# í˜ë¥´ì†Œë‚˜ (Persona)
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ë‚´ë©´ì„ ê¹Šì´ ì´í•´í•˜ê³  ê³µê°í•˜ëŠ” 'ë””ì§€í„¸ ì¹˜ë£Œ ë ˆí¬ë¦¬ì—ì´ì…˜ ì „ë¬¸ê°€'ì…ë‹ˆë‹¤.

# ì»¨í…ìŠ¤íŠ¸: ì‚¬ìš©ì í”„ë¡œí•„ (Context: User Profile)
ì•„ë˜ëŠ” ì‚¬ìš©ìì˜ í˜„ì¬ ìƒíƒœë¥¼ ë¶„ì„í•œ ë°ì´í„°ì…ë‹ˆë‹¤. ì´ ì •ë³´ëŠ” ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•  ê°€ì´ë“œë¼ì¸ì…ë‹ˆë‹¤.
{fsc_summary}
{pssr_summary}
{mp_summary}
{dls_summary}
{ip_summary} 

{hard_constraints}

# ì´ë¯¸ì§€ ë¶„ì„ ì§€ì‹œ (Image Analysis Directive)
ì´ì œ ì´ ì‚¬ìš©ìê°€ 'ì¦ê±°ì› ë˜ ìˆœê°„'ìœ¼ë¡œ ì§ì ‘ ì„ íƒí•œ ì•„ë˜ ì‚¬ì§„ë“¤ì„ ë¶„ì„í•´ ì£¼ì„¸ìš”. ì‚¬ì§„ ì†ì˜ ëª…í™•í•œ ì‚¬ë¬¼ì´ë‚˜ í™œë™ë¿ë§Œ ì•„ë‹ˆë¼, ì „ì²´ì ì¸ ë¶„ìœ„ê¸°, ìƒ‰ê°, ë¹›, êµ¬ë„, ì§ˆê° ë“±ì—ì„œ ëŠê»´ì§€ëŠ” ê°ì„±ì ì¸ ë‹¨ì„œë¥¼ í¬ì°©í•´ ì£¼ì„¸ìš”.

# í•µì‹¬ ê³¼ì œ ë° ê²°ê³¼ë¬¼ í˜•ì‹ (Core Task & Output Format)
ìœ„ì˜ ì‚¬ìš©ì í”„ë¡œí•„ê³¼ ì´ë¯¸ì§€ ë¶„ì„ì„ ì¢…í•©í•˜ì—¬, ì‚¬ìš©ìì—ê²Œ ë§ì¶¤í˜• ì·¨ë¯¸ ì¶”ì²œ ë©”ì‹œì§€ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”. ê²°ê³¼ë¬¼ì€ ì•„ë˜ ë‘ ë¶€ë¶„ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ì•¼ í•˜ë©°, ë°˜ë“œì‹œ ì§€ì •ëœ í˜•ì‹ì„ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤.

### 1. ì‚¬ìš©ì ë¶„ì„ ìš”ì•½
"ì•ˆë…•í•˜ì„¸ìš”, ì‚¬ìš©ìë‹˜ì˜ ë‚´ë©´ ê¹Šì€ ê³³ì„ ì´í•´í•˜ê³  ê³µê°í•˜ëŠ” ë””ì§€í„¸ ì¹˜ë£Œ ë ˆí¬ë¦¬ì—ì´ì…˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤." ë¼ëŠ” ë¬¸ì¥ìœ¼ë¡œ ì‹œì‘í•´ì£¼ì„¸ìš”.
ê·¸ ë‹¤ìŒ, [ì‚¬ìš©ì í”„ë¡œí•„]ê³¼ [ì´ë¯¸ì§€ ë¶„ì„] ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ íŒŒì•…í•œ ì‚¬ìš©ìì˜ ì„±í–¥, ì ì¬ëœ ìš•êµ¬, ì„ í˜¸ë„ ë“±ì„ 1~2 ë¬¸ë‹¨ìœ¼ë¡œ ìš”ì•½í•˜ì—¬ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
ì˜ˆì‹œ: "ì‚¬ìš©ìë‹˜ì˜ ì†Œì¤‘í•œ 'ì¦ê±°ì› ë˜ ìˆœê°„' ì‚¬ì§„ê³¼ í˜„ì¬ í”„ë¡œí•„ì„ ë©´ë°€íˆ ì‚´í´ë³´ì•˜ìŠµë‹ˆë‹¤. ê°•í•œ ì—ë„ˆì§€ì™€ ìê¸° ê³„ë°œ ì˜ì§€, ê·¸ë¦¬ê³  ì§„ì •í•œ 'ì—°ê²°'ì„ í–¥í•œ ê°ˆë§ì„ ëŠë‚„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤..."

### 2. ë§ì¶¤ ì·¨ë¯¸ ì œì•ˆ (3ê°€ì§€)
ë¶„ì„ ìš”ì•½ì— ì´ì–´ì„œ, ì´ ì‚¬ìš©ìê°€ ì§€ê¸ˆ ë°”ë¡œ ì‹œì‘í•  ìˆ˜ ìˆëŠ” ë§ì¶¤í˜• ì·¨ë¯¸ 3ê°€ì§€ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.
ê° ì·¨ë¯¸ëŠ” ë‹¤ìŒ í˜•ì‹ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•˜ì—¬ ì„¤ëª…í•´ì•¼ í•©ë‹ˆë‹¤. **`###`ë‚˜ `**` ê°™ì€ ë§ˆí¬ë‹¤ìš´ ë¬¸ë²•ì€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.**

---
[ì²« ë²ˆì§¸ ì¶”ì²œ ì·¨ë¯¸]
ì—¬ê¸°ì— ì·¨ë¯¸ì˜ ì´ë¦„ì„ ì ì–´ì£¼ì„¸ìš”.

[ì¶”ì²œ ì´ìœ ]
ì—¬ê¸°ì— ì™œ ì´ ì·¨ë¯¸ê°€ ì‚¬ìš©ìì—ê²Œ ì í•©í•œì§€, í”„ë¡œí•„ê³¼ ì‚¬ì§„ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”. `ìì—°(0.75)`ì™€ ê°™ì€ ë‚´ë¶€ ë¶„ì„ ì ìˆ˜ëŠ” ì ˆëŒ€ ë…¸ì¶œí•˜ì§€ ë§ˆì„¸ìš”.

[ë¶€ë“œëŸ¬ìš´ ì²«ê±¸ìŒ]
ì—¬ê¸°ì— ì‚¬ìš©ìê°€ ë¶€ë‹´ ì—†ì´ ì‹œì‘í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ì²« í–‰ë™ì„ ì´ˆëŒ€í•˜ëŠ” ë§íˆ¬ë¡œ ì œì•ˆí•´ì£¼ì„¸ìš”.

---
(ë‘ ë²ˆì§¸, ì„¸ ë²ˆì§¸ ì·¨ë¯¸ë„ ìœ„ì™€ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ ë°˜ë³µ)
"""
    print("Gemini í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ.")
    return prompt

# 2-1. ì·¨ë¯¸ ì¶”ì²œì— ì‚¬ìš©ë  ë„êµ¬(Tool) ì •ì˜
@tool
def analyze_survey_tool(survey_json_string: str) -> dict:
    """[ìˆ˜ì •] ì‚¬ìš©ìì˜ ì„¤ë¬¸ ì‘ë‹µ(JSON ë¬¸ìì—´)ì„ ì…ë ¥ë°›ì•„, IP í”„ë¡œí•„ì´ í¬í•¨ëœ ìˆ˜ì¹˜ì  ì„±í–¥ í”„ë¡œí•„(ë”•ì…”ë„ˆë¦¬)ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    logging.info("--- ğŸ“Š 'ì„¤ë¬¸ ë¶„ì„ ì „ë¬¸ê°€'ê°€ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤. (IP í”„ë¡œí•„ í¬í•¨) ---")
    try:
        responses = json.loads(survey_json_string)

        # --- ìƒˆë¡œìš´ calculate_user_profile_normalized ë¡œì§ ì‹œì‘ ---
        
        # features ë”•ì…”ë„ˆë¦¬ì— 'IP' (Interest Profile) í‚¤ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
        features = {'FSC': {}, 'PSSR': {}, 'MP': {}, 'DLS': {}, 'IP': {}}
        
        def to_int(q_num_str):
            return responses.get(q_num_str)

        # --- FSC (ê¸°ì¡´ê³¼ ë™ì¼) ---
        features['FSC']['time_availability'] = normalize(to_int('1'), 1, 4)
        features['FSC']['financial_budget'] = normalize(to_int('2'), 1, 4)
        features['FSC']['energy_level'] = normalize(to_int('3'), 1, 5)
        features['FSC']['mobility'] = normalize(to_int('4'), 1, 5)
        features['FSC']['has_physical_constraints'] = True if to_int('5') in [1, 2, 3] else False
        features['FSC']['has_housing_constraints'] = True if to_int('12') in [2, 3, 4] else False
        features['FSC']['preferred_space'] = 'indoor' if to_int('6') == 1 else 'outdoor'

        # --- PSSR (ê¸°ì¡´ê³¼ ë™ì¼) ---
        q13 = to_int('13') or 3; q14_r = 6 - (to_int('14') or 3); q16 = to_int('16') or 3
        self_criticism_raw = (q13 + q14_r + q16) / 3
        features['PSSR']['self_criticism_score'] = normalize(self_criticism_raw, 1, 5)
        q15 = to_int('15') or 3; q18 = to_int('18') or 3; q20 = to_int('20') or 3
        social_anxiety_raw = (q15 + q18 + q20) / 3
        features['PSSR']['social_anxiety_score'] = normalize(social_anxiety_raw, 1, 5)
        features['PSSR']['isolation_level'] = normalize(to_int('21'), 1, 5)
        features['PSSR']['structure_preference_score'] = normalize(to_int('27'), 1, 5)
        features['PSSR']['avoidant_coping_score'] = normalize(to_int('29'), 1, 5)

        # --- MP (í•œêµ­ì–´ ê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸) ---
        motivation_map = {1: 'ì„±ì·¨', 2: 'íšŒë³µ', 3: 'ì—°ê²°', 4: 'í™œë ¥'}
        features['MP']['core_motivation'] = motivation_map.get(to_int('31'))
        features['MP']['value_profile'] = {'knowledge': normalize(to_int('33'), 1, 5), 'stability': normalize(to_int('34'), 1, 5),'relationship': normalize(to_int('35'), 1, 5), 'health': normalize(to_int('36'), 1, 5),'creativity': normalize(to_int('37'), 1, 5), 'control': normalize(to_int('38'), 1, 5),}
        features['MP']['process_orientation_score'] = normalize(6 - (to_int('41') or 3), 1, 5)

        # --- DLS (í•œêµ­ì–´ ê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸) ---
        sociality_map = {1: 'ë‹¨ë…í˜•', 2: 'ë³‘ë ¬í˜•', 3: 'ì €ê°•ë„ ìƒí˜¸ì‘ìš©í˜•', 4: 'ê³ ê°•ë„ ìƒí˜¸ì‘ìš©í˜•'}
        features['DLS']['preferred_sociality_type'] = sociality_map.get(to_int('39'))
        group_size_map = {1: '1:1', 2: 'ì†Œê·œëª¨ ê·¸ë£¹', 3: 'ëŒ€ê·œëª¨ ê·¸ë£¹'}
        features['DLS']['preferred_group_size'] = group_size_map.get(to_int('40'))
        features['DLS']['autonomy_preference_score'] = normalize(to_int('42'), 1, 5)
        
        # --- â˜…â˜…â˜… ì¶”ê°€ëœ IP (Interest Profile) ë¡œì§ â˜…â˜…â˜… ---
        features['IP']['nature_interest'] = normalize(to_int('43'), 1, 5)
        features['IP']['craft_interest'] = normalize(to_int('44'), 1, 5)
        features['IP']['intellect_interest'] = normalize(to_int('45'), 1, 5)
        features['IP']['art_interest'] = normalize(to_int('46'), 1, 5)
        features['IP']['activity_interest'] = normalize(to_int('47'), 1, 5)
        # --- â˜…â˜…â˜… ì—¬ê¸°ê¹Œì§€ â˜…â˜…â˜… ---
        
        logging.info("--- âœ… ì„¤ë¬¸ ë¶„ì„ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ---")
        return features
    except Exception as e:
        logging.error(f"ì„¤ë¬¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        return {"error": f"ì„¤ë¬¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"}

@tool
def analyze_photo_tool(image_paths: list[str], survey_profile: dict) -> str:
    """[ìˆ˜ì •] ì‚¬ìš©ìì˜ ì‚¬ì§„(image_paths)ê³¼ ì„¤ë¬¸ í”„ë¡œí•„(survey_profile)ì„ ëª¨ë‘ ì…ë ¥ë°›ì•„,
    ë‘ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ì·¨ë¯¸ ì¶”ì²œ ë©”ì‹œì§€ë¥¼ ìƒì„±í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤."""
    from PIL import Image
    
    # 1. í”„ë¡œí•„ì„ ê¸°ë°˜ìœ¼ë¡œ Geminiì—ê²Œ ë³´ë‚¼ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    try:
        prompt_text = generate_prompt(survey_profile)
    except Exception as e:
        logging.error(f"í”„ë¡œí•„ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        return f"ì˜¤ë¥˜: ì‚¬ìš©ì í”„ë¡œí•„ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}"

    # 2. ì´ë¯¸ì§€ê°€ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    if not image_paths:
        logging.info("--- ğŸ–¼ï¸ ë¶„ì„í•  ì‚¬ì§„ì´ ì—†ì–´ ì‚¬ì§„ ë¶„ì„ ë‹¨ê³„ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤. ---")
        # ì‚¬ì§„ì´ ì—†ì–´ë„, ì„¤ë¬¸ ê¸°ë°˜ ì¶”ì²œì€ ê°€ëŠ¥í•˜ë¯€ë¡œ í”„ë¡¬í”„íŠ¸ë§Œ ì „ë‹¬í•©ë‹ˆë‹¤.
        image_parts = ["\n# ì¶”ê°€ ì •ë³´: ì‚¬ìš©ìê°€ ì œê³µí•œ ì‚¬ì§„ì´ ì—†ìŠµë‹ˆë‹¤."]
    else:
        logging.info(f"--- ğŸ“¸ 'ë””ì§€í„¸ ì¹˜ë£Œ ë ˆí¬ë¦¬ì—ì´ì…˜ ì „ë¬¸ê°€'ê°€ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤. (ì´ë¯¸ì§€ {len(image_paths)}ê°œ) ---")
        image_parts = []

    # 3. ì´ë¯¸ì§€ íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    try:
        for path in image_paths:
            try:
                img = Image.open(path)
                # Pillowê°€ ì§€ì›í•˜ì§€ë§Œ Geminiê°€ ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹ì„ ëŒ€ë¹„í•´ format í™•ì¸
                if img.format.upper() == 'MPO':
                    # MPO íŒŒì¼ì˜ ê²½ìš°, ì²« ë²ˆì§¸ í”„ë ˆì„(ì¼ë°˜ì ìœ¼ë¡œ JPEG)ì„ ì‚¬ìš©
                    img.seek(0)
                    # MPOì—ì„œ ì¶”ì¶œí•œ ì´ë¯¸ì§€ëŠ” format ì†ì„±ì´ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, JPEGë¡œ ê°„ì£¼í•˜ê³  ì¶”ê°€
                    image_parts.append(img.copy()) # copy()ë¡œ ì•ˆì „í•˜ê²Œ ì¶”ê°€
                    logging.info(f"MPO í˜•ì‹ íŒŒì¼ì—ì„œ JPEG í”„ë ˆì„ì„ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤: {path}")
                elif img.format.upper() in ['JPEG', 'PNG', 'WEBP', 'HEIC', 'HEIF']:
                    image_parts.append(img)
                else:
                    logging.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹({img.format})ì„ ê±´ë„ˆëœë‹ˆë‹¤: {path}")
            except Exception as img_e:
                logging.warning(f"ì´ë¯¸ì§€ íŒŒì¼ì„ ì—¬ëŠ” ë° ì‹¤íŒ¨í•˜ì—¬ ê±´ë„ˆëœë‹ˆë‹¤: {path}, ì˜¤ë¥˜: {img_e}")
        
        # 4. Gemini ëª¨ë¸ì„ í˜¸ì¶œí•©ë‹ˆë‹¤.
        model = genai.GenerativeModel('gemini-2.5-flash')
        # [ìˆ˜ì •] generate_promptë¡œ ìƒì„±í•œ í”„ë¡¬í”„íŠ¸ì™€ ì´ë¯¸ì§€ íŒŒíŠ¸ë¥¼ í•¨ê»˜ ì „ë‹¬
        response = model.generate_content([prompt_text] + image_parts) 
        
        logging.info("--- âœ… ìµœì¢… ì¶”ì²œ ë©”ì‹œì§€ ìƒì„±ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ---")
        return response.text
    except Exception as e:
        logging.error(f"Gemini ì¶”ì²œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        return f"ì˜¤ë¥˜: Geminië¥¼ í†µí•œ ìµœì¢… ì¶”ì²œ ìƒì„± ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

# 2-2. ì·¨ë¯¸ ì¶”ì²œ StateGraph ì •ì˜ [ìˆ˜ì •]
class HobbyAgentState(TypedDict):
    survey_data: dict
    image_paths: List[str]
    survey_profile: dict
    # [ì‚­ì œ] survey_summary: str
    # [ì‚­ì œ] photo_analysis: str
    final_recommendation: str

def analyze_survey_node(state: HobbyAgentState):
    """ì„¤ë¬¸ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì •ëŸ‰ í”„ë¡œí•„ì„ ìƒì„±í•˜ëŠ” ë…¸ë“œ"""
    survey_json_string = json.dumps(state["survey_data"], ensure_ascii=False)
    survey_profile = analyze_survey_tool.invoke({"survey_json_string": survey_json_string})
    return {"survey_profile": survey_profile}

def analyze_photo_node(state: HobbyAgentState):
    """[ìˆ˜ì •] ì‚¬ì§„ê³¼ ì„¤ë¬¸ í”„ë¡œí•„ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ì¶”ì²œì„ ìƒì„±í•˜ëŠ” ë…¸ë“œ"""
    final_recommendation = analyze_photo_tool.invoke({
        "image_paths": state.get("image_paths", []),
        "survey_profile": state["survey_profile"]
    })
    return {"final_recommendation": final_recommendation}

# 2-3. ì·¨ë¯¸ ì¶”ì²œ StateGraph ì»´íŒŒì¼ [ìˆ˜ì •]
hobby_graph_builder = StateGraph(HobbyAgentState)
hobby_graph_builder.add_node("analyze_survey", analyze_survey_node)
hobby_graph_builder.add_node("analyze_photo_and_recommend", analyze_photo_node) # [ìˆ˜ì •] ë…¸ë“œ ì´ë¦„ ë³€ê²½

hobby_graph_builder.set_entry_point("analyze_survey")
hobby_graph_builder.add_edge("analyze_survey", "analyze_photo_and_recommend") # [ìˆ˜ì •] ì—£ì§€ ì—°ê²°
hobby_graph_builder.add_edge("analyze_photo_and_recommend", END) # [ìˆ˜ì •] ì—£ì§€ ì—°ê²°

hobby_supervisor_agent = hobby_graph_builder.compile()

# 2-4. ë§ˆìŠ¤í„° ì—ì´ì „íŠ¸ê°€ í˜¸ì¶œí•  í•¨ìˆ˜
def call_multimodal_hobby_agent(state: MasterAgentState):
    """'StateGraph ê¸°ë°˜ ì·¨ë¯¸ ì¶”ì²œ ì—ì´ì „íŠ¸'ë¥¼ í˜¸ì¶œí•˜ê³  ê²°ê³¼ë¥¼ ë°›ì•„ì˜¤ëŠ” ë…¸ë“œ"""
    logging.info("--- CALLING: StateGraph Hobby Supervisor Agent (IP Profile Ver.) ---")

    user_input = state["user_input"]
    survey_data = user_input.get("survey", {})
    image_paths = user_input.get("image_paths", [])

    input_data = {"survey_data": survey_data, "image_paths": image_paths}
    
    final_state = hobby_supervisor_agent.invoke(input_data, config={"recursion_limit": 10})
    
    final_answer = final_state.get("final_recommendation", "ì˜¤ë¥˜: ìµœì¢… ì¶”ì²œì„ ìƒì„±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                
    return {"final_answer": final_answer}


# 5-4. ë§ˆìŠ¤í„° ì—ì´ì „íŠ¸ ê·¸ë˜í”„ ì¡°ë¦½ ë° ì»´íŒŒì¼
master_graph_builder = StateGraph(MasterAgentState)

master_graph_builder.add_node("router", route_request)
master_graph_builder.add_node("meeting_matcher", call_meeting_matching_agent)
master_graph_builder.add_node("hobby_recommender", call_multimodal_hobby_agent)
master_graph_builder.add_node("general_searcher", call_general_search_agent) # ìƒˆ ë…¸ë“œ ì¶”ê°€

master_graph_builder.set_entry_point("router")

master_graph_builder.add_conditional_edges(
    "router", 
    lambda state: state['route'],
    {
        "meeting_matching": "meeting_matcher", 
        "hobby_recommendation": "hobby_recommender",
        "general_search": "general_searcher" # ìƒˆ ê²½ë¡œ ì—°ê²°
    }
)

master_graph_builder.add_edge("meeting_matcher", END)
master_graph_builder.add_edge("hobby_recommender", END)
master_graph_builder.add_edge("general_searcher", END) # ìƒˆ ë…¸ë“œ ì¢…ë£Œì  ì—°ê²°

master_agent = master_graph_builder.compile()


# --- 6. API ì—”ë“œí¬ì¸íŠ¸ ì •ì˜ ---
class UserRequest(BaseModel):
    user_input: dict

@app.post("/agent/invoke")
async def invoke_agent(request: UserRequest):
    try:
        input_data = {"user_input": request.user_input}
        result = master_agent.invoke(input_data, {"recursion_limit": 5}) # ë§ˆìŠ¤í„° ì—ì´ì „íŠ¸ì—ë„ ì•ˆì „ì¥ì¹˜ ì¶”ê°€
        return {"final_answer": result.get("final_answer", "ì˜¤ë¥˜: ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")}
    except Exception as e:
        logging.error(f"Agent ì‹¤í–‰ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"AI ì—ì´ì „íŠ¸ ì²˜ë¦¬ ì¤‘ ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

class NewMeeting(BaseModel):
    meeting_id: str
    title: str
    description: str
    time: str
    location: str

@app.post("/meetings/add")
async def add_meeting_to_pinecone(meeting: NewMeeting):
    try:
        logging.info(f"--- Pineconeì— ìƒˆë¡œìš´ ëª¨ì„ ì¶”ê°€ ì‹œì‘ (ID: {meeting.meeting_id}) ---")
        meeting_index_name = os.getenv("PINECONE_INDEX_NAME_MEETING")
        if not meeting_index_name: raise ValueError("'.env' íŒŒì¼ì— PINECONE_INDEX_NAME_MEETINGì´(ê°€) ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        embedding_function = OpenAIEmbeddings(model='text-embedding-3-large')
        vector_store = PineconeVectorStore.from_existing_index(index_name=meeting_index_name, embedding=embedding_function)
        
        full_text = f"ì œëª©: {meeting.title}\nì„¤ëª…: {meeting.description}\nì‹œê°„: {meeting.time}\nì¥ì†Œ: {meeting.location}"
        metadata = {
            "title": meeting.title, 
            "description": meeting.description, 
            "time": meeting.time, 
            "location": meeting.location,
            "meeting_id": meeting.meeting_id 
        }
        
        vector_store.add_texts(texts=[full_text], metadatas=[metadata], ids=[meeting.meeting_id])
        
        logging.info(f"--- Pineconeì— ëª¨ì„ ì¶”ê°€ ì„±ê³µ (ID: {meeting.meeting_id}) ---")
        return {"status": "success", "message": f"ëª¨ì„(ID: {meeting.meeting_id})ì´ ì„±ê³µì ìœ¼ë¡œ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤."}
    except Exception as e:
        logging.error(f"Pinecone ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Pineconeì— ëª¨ì„ì„ ì¶”ê°€í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@app.delete("/meetings/delete/{meeting_id}")
async def delete_meeting_from_pinecone(meeting_id: str):
    try:
        logging.info(f"--- Pineconeì—ì„œ ëª¨ì„ ì‚­ì œ ì‹œì‘ (ID: {meeting_id}) ---")
        meeting_index_name = os.getenv("PINECONE_INDEX_NAME_MEETING")
        if not meeting_index_name: raise ValueError("'.env' íŒŒì¼ì— PINECONE_INDEX_NAME_MEETINGì´(ê°€) ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        embedding_function = OpenAIEmbeddings(model='text-embedding-3-large')
        vector_store = PineconeVectorStore.from_existing_index(index_name=meeting_index_name, embedding=embedding_function)
        
        vector_store.delete(ids=[meeting_id])
        
        logging.info(f"--- Pineconeì—ì„œ ëª¨ì„ ì‚­ì œ ì„±ê³µ (ID: {meeting_id}) ---")
        return {"status": "success", "message": f"ëª¨ì„(ID: {meeting_id})ì´ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."}
    except Exception as e:
        logging.error(f"Pinecone ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Pineconeì—ì„œ ëª¨ì„ì„ ì‚­ì œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.: {str(e)}")
