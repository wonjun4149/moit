# main_final.py (ì •ë³´ íë¦„ ìµœì í™”, ìµœì¢… ì™„ì„±ë³¸)

# --- 1. ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ import ---
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import os
from dotenv import load_dotenv
import json
from typing import List, TypedDict
import logging
from fastapi.middleware.cors import CORSMiddleware

# --- 2. ë¡œê¹… ê¸°ë³¸ ì„¤ì • ---
logging.basicConfig(level=logging.INFO, format='%(levelname)s:     %(message)s')

# --- 3. LangChain, LangGraph ë° AI ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ import ---
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_pinecone import PineconeVectorStore
from langgraph.prebuilt import create_react_agent
from langgraph.graph import StateGraph, END
from langchain_core.tools import tool
import google.generativeai as genai
from langchain_core.documents import Document

# --- 4. í™˜ê²½ ì„¤ì • ---
load_dotenv()

app = FastAPI(
    title="MOIT AI Agent Server v3.1",
    description="ì •ë³´ íë¦„ì´ ìµœì í™”ëœ ìµœì¢… ê°ë…ê´€ ì‹œìŠ¤í…œ",
    version="3.1.0",
)

# --- CORS ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€ ---
origins = ["*"]
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- AI ëª¨ë¸ ë° API í‚¤ ì„¤ì • ---
try:
    gemini_api_key = os.getenv("GOOGLE_API_KEY")
    if not gemini_api_key:
        logging.warning("GOOGLE_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‚¬ì§„ ë¶„ì„ ê¸°ëŠ¥ì´ ì‘ë™í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        genai.configure(api_key=gemini_api_key)
except Exception as e:
    logging.warning(f"Gemini API í‚¤ ì„¤ì • ì‹¤íŒ¨: {e}")

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.4)
llm_for_meeting = ChatOpenAI(model="gpt-4o-mini", temperature=0)


# --- 5. [ë³µì›] 'ëª¨ì„ ë§¤ì¹­ ì „ë¬¸ê°€(Self-RAG SubGraph)' ì „ì²´ ì •ì˜ ---

# 5-1. ëª¨ì„ ë§¤ì¹­ ì „ë¬¸ê°€ì˜ State ì •ì˜
class MeetingMatchingState(TypedDict):
    title: str
    description: str
    time: str
    location: str
    query: str
    context: List[Document]
    answer: str
    is_helpful: str
    rewrite_count: int

# 5-2. ëª¨ì„ ë§¤ì¹­ ì „ë¬¸ê°€ì˜ ê° ë…¸ë“œ(ê¸°ëŠ¥)ë“¤ ì •ì˜
def prepare_query_node(state: MeetingMatchingState):
    logging.info("--- (ëª¨ì„ ë§¤ì¹­) 1. ê²€ìƒ‰ì–´ ìƒì„± ë…¸ë“œ ---")
    prompt = ChatPromptTemplate.from_template(
        "ë‹¹ì‹ ì€ ì‚¬ìš©ìê°€ ë§Œë“¤ë ¤ëŠ” ëª¨ì„ì˜ ìƒì„¸ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, PineconeDBì—ì„œ ìœ ì‚¬ ëª¨ì„ì„ ì°¾ê¸° ìœ„í•œ ìµœì ì˜ ê²€ìƒ‰ì–´ë¥¼ ìƒì„±í•˜ëŠ” AIì…ë‹ˆë‹¤. "
        "ì•„ë˜ ì •ë³´ë¥¼ ì¡°í•©í•˜ì—¬, ê°€ì¥ í•µì‹¬ì ì¸ í‚¤ì›Œë“œê°€ ë‹´ê¸´ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ í˜•íƒœì˜ ê²€ìƒ‰ì–´ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.\n\n"
        "[ëª¨ì„ ì •ë³´]\n"
        "ì œëª©: {title}\n"
        "ì„¤ëª…: {description}\n"
        "ì‹œê°„: {time}\n"
        "ì¥ì†Œ: {location}\n\n"
        "[ì‘ì„± ê°€ì´ë“œ]\n"
        "- 'ì œëª©'ê³¼ 'ì„¤ëª…'ì— ë‹´ê¸´ í•µì‹¬ í™œë™ì´ë‚˜ ì£¼ì œë¥¼ ê°€ì¥ ì¤‘ìš”í•œ í‚¤ì›Œë“œë¡œ ì‚¼ìœ¼ì„¸ìš”.\n"
        "- 'ì¥ì†Œ'ëŠ” ì¤‘ìš”í•œ ì°¸ê³  ì •ë³´ì´ì§€ë§Œ, ë„ˆë¬´ êµ¬ì²´ì ì¸ ì¥ì†Œ ì´ë¦„ë³´ë‹¤ëŠ” ë” ë„“ì€ ì§€ì—­(ì˜ˆ: 'ì„œìš¸', 'ê°•ë‚¨')ì„ í¬í•¨í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n"
        "- 'ì‹œê°„' ì •ë³´ëŠ” ê²€ìƒ‰ì–´ì— í¬í•¨í•˜ì§€ ì•Šì•„ë„ ì¢‹ìŠµë‹ˆë‹¤."
    )
    chain = prompt | llm_for_meeting | StrOutputParser()
    better_query = chain.invoke({
        "title": state.get("title", ""),
        "description": state.get("description", ""),
        "time": state.get("time", ""),
        "location": state.get("location", "")
    })
    # ì´ˆê¸° ìƒíƒœë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
    return {
        "title": state.get("title", ""),
        "description": state.get("description", ""),
        "time": state.get("time", ""),
        "location": state.get("location", ""),
        "query": better_query,
        "rewrite_count": 0
    }

def retrieve_node(state: MeetingMatchingState):
    logging.info(f"--- (ëª¨ì„ ë§¤ì¹­) 2. ê²€ìƒ‰ ë…¸ë“œ ({state.get('rewrite_count', 0)+1}ë²ˆì§¸) ---")
    meeting_index_name = os.getenv("PINECONE_INDEX_NAME_MEETING")
    embedding_function = OpenAIEmbeddings(model='text-embedding-3-large')
    vector_store = PineconeVectorStore.from_existing_index(index_name=meeting_index_name, embedding=embedding_function)
  
    retriever = vector_store.as_retriever(
        search_type="similarity_score_threshold", 
        search_kwargs={'score_threshold': 0.7, 'k': 2} 
    )
    context = retriever.invoke(state["query"])
    return {"context": context}


def generate_node(state: MeetingMatchingState):
    logging.info("--- (ëª¨ì„ ë§¤ì¹­) 3. ë‹µë³€ ìƒì„± ë…¸ë“œ ---")
    context = state["context"]
    original_query = f"ì œëª©: {state['title']}, ì„¤ëª…: {state['description']}"
    context_str = "\n".join([f"ëª¨ì„ ID: {doc.metadata.get('meeting_id', 'N/A')}, ì œëª©: {doc.metadata.get('title', 'N/A')}, ë‚´ìš©: {doc.page_content}" for doc in context])
    if not context:
        context_str = "ìœ ì‚¬í•œ ëª¨ì„ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
    
    prompt_str = """ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë§¤ìš° ì—„ê²©í•˜ê²Œ ë¶„ì„í•˜ì—¬ ìœ ì‚¬í•œ ëª¨ì„ì„ ì¶”ì²œí•˜ëŠ” MOIT í”Œë«í¼ì˜ AIì…ë‹ˆë‹¤.

[ê²€ìƒ‰ëœ ìœ ì‚¬ ëª¨ì„ ì •ë³´]
{context}

[ì‚¬ìš©ìê°€ ë§Œë“¤ë ¤ëŠ” ëª¨ì„ ì •ë³´]
{query}

[ì§€ì‹œì‚¬í•­]
1. [ê²€ìƒ‰ëœ ìœ ì‚¬ ëª¨ì„ ì •ë³´]ê°€ "ìœ ì‚¬í•œ ëª¨ì„ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."ê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì•„ë˜ ì‘ì—…ì„ ìˆ˜í–‰í•˜ì„¸ìš”.
2. [ê²€ìƒ‰ëœ ìœ ì‚¬ ëª¨ì„ ì •ë³´]ì™€ [ì‚¬ìš©ìê°€ ë§Œë“¤ë ¤ëŠ” ëª¨ì„ ì •ë³´]ë¥¼ ë¹„êµí•˜ì—¬, ì •ë§ë¡œ ìœ ì‚¬í•˜ë‹¤ê³  íŒë‹¨ë˜ëŠ” ëª¨ì„ë§Œ ê³¨ë¼ì£¼ì„¸ìš”.
3. ì‚¬ìš©ìê°€ í˜¹í•  ë§Œí•œ ë§¤ë ¥ì ì¸ ì¶”ì²œ ë¬¸êµ¬ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
4. ìµœì¢… ë‹µë³€ì€ ë°˜ë“œì‹œ ì•„ë˜ì™€ ê°™ì€ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤. ì¶”ê°€ì ì¸ ì„¤ëª…ì€ ì ˆëŒ€ ë¶™ì´ì§€ ë§ˆì„¸ìš”.

ë‹¹ì‹ ì˜ ì „ì²´ ì‘ë‹µì€ ë‹¤ë¥¸ ì–´ë–¤ í…ìŠ¤íŠ¸ë„ ì—†ì´, ì˜¤ì§ '{{'ë¡œ ì‹œì‘í•´ì„œ '}}'ë¡œ ëë‚˜ëŠ” ìœ íš¨í•œ JSON ê°ì²´ì—¬ì•¼ í•©ë‹ˆë‹¤.

[JSON í˜•ì‹]
{{
    "summary": "ìš”ì•½ ì¶”ì²œ ë¬¸êµ¬ (ì˜ˆ: 'ì´ëŸ° ëª¨ì„ì€ ì–´ë– ì„¸ìš”? ë¹„ìŠ·í•œ ì£¼ì œì˜ ëª¨ì„ì´ ì´ë¯¸ í™œë°œí•˜ê²Œ í™œë™ ì¤‘ì´ì—ìš”!')",
    "recommendations": [
        {{
            "meeting_id": "ì¶”ì²œ ëª¨ì„ì˜ ID",
            "title": "ì¶”ì²œ ëª¨ì„ì˜ ì œëª©"
        }}
    ]
}}

[ê²€ìƒ‰ëœ ëª¨ì„ì´ ì—†ëŠ” ê²½ìš° JSON í˜•ì‹]
{{
    "summary": "",
    "recommendations": []
}}
"""
    prompt = ChatPromptTemplate.from_template(prompt_str)
    chain = prompt | llm_for_meeting | StrOutputParser()
    answer = chain.invoke({"context": context_str, "query": original_query})
    return {"answer": answer}

def check_helpfulness_node(state: MeetingMatchingState):
    logging.info("--- (ëª¨ì„ ë§¤ì¹­) 4. ìœ ìš©ì„± ê²€ì¦ ë…¸ë“œ ---")
    prompt = ChatPromptTemplate.from_template("ë‹¹ì‹ ì€ AIê°€ ìƒì„±í•œ ì¶”ì²œì´ ì‚¬ìš©ìì—ê²Œ ì •ë§ ë„ì›€ì´ ë˜ëŠ”ì§€ íŒë‹¨í•˜ëŠ” ê²€ì¦ AIì…ë‹ˆë‹¤. 'helpful' ë˜ëŠ” 'unhelpful' ë‘˜ ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”.\n\n[AIì˜ ì¶”ì²œ ë‚´ìš©]\n{answer}\n\n[ì‚¬ìš©ìì˜ ì›ë˜ ìš”ì²­]\nì œëª©: {title}\nì„¤ëª…: {description}\n\n[ê²€ì¦ ê¸°ì¤€]\n- [AI ë‹µë³€]ì˜ `recommendations` ë°°ì—´ì´ ë¹„ì–´ìˆì§€ ì•Šì€ì§€ í™•ì¸í•˜ì„¸ìš”.\n- [AI ë‹µë³€]ì˜ `summary`ê°€ ê¸ì •ì ì¸ ì¶”ì²œ ë¬¸êµ¬ì¸ì§€ í™•ì¸í•˜ì„¸ìš”. (ì˜ˆ: 'ë¹„ìŠ·í•œ ëª¨ì„ì´ ìˆì–´ìš”' ë“±)\n- ìœ„ ë‘ ì¡°ê±´ì´ ëª¨ë‘ ì¶©ì¡±ë˜ê³ , ì¶”ì²œëœ ëª¨ì„ì˜ ì£¼ì œê°€ [ì›ë³¸ ì§ˆë¬¸]ê³¼ ê´€ë ¨ì´ ìˆë‹¤ë©´ 'helpful'ì…ë‹ˆë‹¤.\n- ê·¸ ì™¸ ëª¨ë“  ê²½ìš°ëŠ” 'unhelpful'ì…ë‹ˆë‹¤.")
    chain = prompt | llm_for_meeting | StrOutputParser()
    # íŒŒì‹± ì˜¤ë¥˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ strip()ê³¼ lower()ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
    raw_helpful = chain.invoke({"answer": state["answer"], "title": state["title"], "description": state["description"]})
    is_helpful = "helpful" if "helpful" in raw_helpful.strip().lower() else "unhelpful"
    return {"is_helpful": is_helpful}

def rewrite_query_node(state: MeetingMatchingState):
    logging.info("--- (ëª¨ì„ ë§¤ì¹­) 5. ê²€ìƒ‰ì–´ ì¬ì‘ì„± ë…¸ë“œ ---")
    prompt = ChatPromptTemplate.from_template("ë‹¹ì‹ ì€ ì´ì „ ê²€ìƒ‰ ê²°ê³¼ê°€ ë§Œì¡±ìŠ¤ëŸ½ì§€ ì•Šì•„ ê²€ìƒ‰ì–´ë¥¼ ì¬ì‘ì„±í•˜ëŠ” AIì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì›ë˜ ì˜ë„ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì´ì „ê³¼ëŠ” ë‹¤ë¥¸ ê´€ì ì˜ ìƒˆë¡œìš´ ê²€ìƒ‰ì–´ë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”.\n\n[ì´ì „ ê²€ìƒ‰ì–´]\n{query}")
    chain = prompt | llm_for_meeting | StrOutputParser()
    new_query = chain.invoke({"query": state["query"]})
    return {"query": new_query, "rewrite_count": state["rewrite_count"] + 1}

def decide_to_continue(state: MeetingMatchingState):
    return "end" if state["rewrite_count"] > 1 or state["is_helpful"] == "helpful" else "continue"

# 5-3. ëª¨ì„ ë§¤ì¹­ ì „ë¬¸ê°€ ê·¸ë˜í”„(SubGraph) ì¡°ë¦½
builder = StateGraph(MeetingMatchingState)
builder.add_node("prepare_query", prepare_query_node)
builder.add_node("retrieve", retrieve_node)
builder.add_node("generate", generate_node)
builder.add_node("check_helpfulness", check_helpfulness_node)
builder.add_node("rewrite_query", rewrite_query_node)
builder.set_entry_point("prepare_query")
builder.add_edge("prepare_query", "retrieve")
builder.add_edge("retrieve", "generate")
builder.add_edge("generate", "check_helpfulness")
builder.add_conditional_edges("check_helpfulness", decide_to_continue, {"continue": "rewrite_query", "end": END})
builder.add_edge("rewrite_query", "retrieve")
meeting_matching_agent = builder.compile()


# --- 'ëª¨ì„ ë§¤ì¹­ ì „ë¬¸ê°€'ë¥¼ Toolë¡œ í¬ì¥í•˜ëŠ” ë¶€ë¶„ ---
@tool
def run_meeting_matching_agent_tool(meeting_info: dict) -> str:
    """
    ì‚¬ìš©ìê°€ ë§Œë“¤ë ¤ëŠ” ëª¨ì„ì˜ ìƒì„¸ ì •ë³´(ë”•ì…”ì…”ë„ˆë¦¬)ë¥¼ ì…ë ¥ë°›ì•„, Self-RAG ê¸°ë°˜ì˜ ì§€ëŠ¥í˜• ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•˜ì—¬ ìœ ì‚¬ ëª¨ì„ì„ ì°¾ì•„ ì¶”ì²œ ê²°ê³¼ë¥¼ JSON ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    logging.info(f"--- ğŸ¤– 'Self-RAG ëª¨ì„ ë§¤ì¹­ ì „ë¬¸ê°€'ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤. ì…ë ¥: {meeting_info} ---")
    try:
        # [â˜…ìˆ˜ì •ëœ í•µì‹¬ ë¶€ë¶„â˜…]
        # ì—ì´ì „íŠ¸ê°€ ëª¨ë“  ë£¨í”„ë¥¼ ë§ˆì¹˜ê³  ë‚´ë†“ì€ ìµœì¢… ë‹µë³€ì„ ê·¸ëŒ€ë¡œ ì‹ ë¢°í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤.
        # ë¶ˆí•„ìš”í•œ 'is_helpful' ì™¸ë¶€ ê²€ì¦ ë¡œì§ì„ ì œê±°í•©ë‹ˆë‹¤.
        final_state = meeting_matching_agent.invoke(meeting_info, {"recursion_limit": 5})
        logging.info("--- âœ… Self-RAG ëª¨ì„ ë§¤ì¹­ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ---")
        
        # ì—ì´ì „íŠ¸ì˜ ìµœì¢… ë‹µë³€ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜
        return final_state.get("answer", json.dumps({"summary": "ì˜¤ë¥˜: ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", "recommendations": []}))
            
    except Exception as e:
        logging.error(f"Self-RAG ëª¨ì„ ë§¤ì¹­ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        return json.dumps({"summary": "ëª¨ì„ ì¶”ì²œ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "recommendations": []})


# ì „ë¬¸ê°€ 2: ì‚¬ì§„ ë¶„ì„ ì „ë¬¸ê°€
@tool
def analyze_photo_tool(image_paths: list[str]) -> str:
    """
    ì‚¬ìš©ìì˜ ì‚¬ì§„(ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸)ì„ ì…ë ¥ë°›ì•„, ê·¸ ì‚¬ëŒì˜ ì„±í–¥, ë¶„ìœ„ê¸°, ì ì¬ì  ê´€ì‹¬ì‚¬ì— ëŒ€í•œ í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    from PIL import Image
    try:
        logging.info(f"--- ğŸ“¸ 'ì‚¬ì§„ ë¶„ì„ ì „ë¬¸ê°€'ê°€ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤. (ì´ë¯¸ì§€ {len(image_paths)}ê°œ) ---")
        model = genai.GenerativeModel('gemini-2.5-flash')
        photo_analysis_prompt_text = "ë‹¹ì‹ ì€ ì‚¬ëŒë“¤ì˜ ì¼ìƒ ì‚¬ì§„ì„ ë³´ê³ , ê·¸ ì‚¬ëŒì˜ ì ì¬ì ì¸ ê´€ì‹¬ì‚¬ì™€ ì„±í–¥ì„ ì¶”ì¸¡í•˜ëŠ” ì‹¬ë¦¬ ë¶„ì„ê°€ì…ë‹ˆë‹¤. [ë¶„ì„í•  ì‚¬ì§„] ì•„ë˜ ì œê³µëœ ì‚¬ì§„ë“¤ [ì§€ì‹œì‚¬í•­] 1. ì‚¬ì§„ë“¤ ì† ì¸ë¬¼, ì‚¬ë¬¼, ë°°ê²½, ë¶„ìœ„ê¸°ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”. 2. ì‚¬ì§„ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì´ ì‚¬ëŒì˜ ì„±í–¥ê³¼ ì ì¬ì ì¸ ê´€ì‹¬ì‚¬ë¥¼ 3~4ê°œì˜ í•µì‹¬ í‚¤ì›Œë“œì™€ í•¨ê»˜ ì„¤ëª…í•´ì£¼ì„¸ìš”. 3. ìµœì¢… ê²°ê³¼ëŠ” ë‹¤ë¥¸ AIê°€ ì´í•´í•˜ê¸° ì‰½ë„ë¡ ê°„ê²°í•œ ë¶„ì„ ë³´ê³ ì„œ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."
        image_parts = [Image.open(path) for path in image_paths]
        response = model.generate_content([photo_analysis_prompt_text] + image_parts)
        logging.info("--- âœ… ì‚¬ì§„ ë¶„ì„ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ---")
        return response.text
    except Exception as e:
        logging.error(f"ì‚¬ì§„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        return f"ì˜¤ë¥˜: ì‚¬ì§„ ë¶„ì„ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

# ì „ë¬¸ê°€ 3: ì„¤ë¬¸ ë¶„ì„ ì „ë¬¸ê°€
def _normalize(value, min_val, max_val):
    if value is None: return None
    return round((value - min_val) / (max_val - min_val), 4)

@tool
def analyze_survey_tool(survey_json_string: str) -> dict:
    """
    ì‚¬ìš©ìì˜ ì„¤ë¬¸ ì‘ë‹µ(JSON ë¬¸ìì—´)ì„ ì…ë ¥ë°›ì•„, ìˆ˜ì¹˜ì ìœ¼ë¡œ ì •ê·œí™”ëœ ì„±í–¥ í”„ë¡œí•„(ë”•ì…”ë„ˆë¦¬)ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    logging.info("--- ğŸ“Š 'ì„¤ë¬¸ ë¶„ì„ ì „ë¬¸ê°€'ê°€ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤. ---")
    try:
        responses = json.loads(survey_json_string)
        features = {'FSC': {}, 'PSSR': {}, 'MP': {}, 'DLS': {}}
        features['FSC']['time_availability'] = _normalize(responses.get('1'), 1, 4)
        features['FSC']['financial_budget'] = _normalize(responses.get('2'), 1, 4)
        features['FSC']['energy_level'] = _normalize(responses.get('3'), 1, 5)
        features['FSC']['mobility'] = _normalize(responses.get('4'), 1, 5)
        features['FSC']['has_physical_constraints'] = True if responses.get('5') in [1, 2, 3] else False
        features['FSC']['has_housing_constraints'] = True if responses.get('12') in [2, 3, 4] else False
        features['FSC']['preferred_space'] = 'indoor' if responses.get('6') == 1 else 'outdoor'
        q13 = responses.get('13', 3); q14_r = 6 - responses.get('14', 3); q16 = responses.get('16', 3)
        self_criticism_raw = (q13 + q14_r + q16) / 3
        features['PSSR']['self_criticism_score'] = _normalize(self_criticism_raw, 1, 5)
        q15 = responses.get('15', 3); q18 = responses.get('18', 3); q20 = responses.get('20', 3)
        social_anxiety_raw = (q15 + q18 + q20) / 3
        features['PSSR']['social_anxiety_score'] = _normalize(social_anxiety_raw, 1, 5)
        features['PSSR']['isolation_level'] = _normalize(responses.get('21'), 1, 5)
        features['PSSR']['structure_preference_score'] = _normalize(responses.get('27'), 1, 5)
        features['PSSR']['avoidant_coping_score'] = _normalize(responses.get('29'), 1, 5)
        motivation_map = {1: 'achievement', 2: 'recovery', 3: 'connection', 4: 'vitality'}
        features['MP']['core_motivation'] = motivation_map.get(responses.get('31'))
        features['MP']['value_profile'] = {'knowledge': _normalize(responses.get('33'), 1, 5),'stability': _normalize(responses.get('34'), 1, 5),'relationship': _normalize(responses.get('35'), 1, 5),'health': _normalize(responses.get('36'), 1, 5),'creativity': _normalize(responses.get('37'), 1, 5),'control': _normalize(responses.get('38'), 1, 5),}
        features['MP']['process_orientation_score'] = _normalize(6 - responses.get('41', 3), 1, 5)
        sociality_map = {1: 'solo', 2: 'parallel', 3: 'low_interaction_group', 4: 'high_interaction_group'}
        features['DLS']['preferred_sociality_type'] = sociality_map.get(responses.get('39'))
        group_size_map = {1: 'one_on_one', 2: 'small_group', 3: 'large_group'}
        features['DLS']['preferred_group_size'] = group_size_map.get(responses.get('40'))
        features['DLS']['autonomy_preference_score'] = _normalize(responses.get('42'), 1, 5)
        logging.info("--- âœ… ì„¤ë¬¸ ë¶„ì„ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ---")
        return features
    except Exception as e:
        logging.error(f"ì„¤ë¬¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        return {"error": f"ì„¤ë¬¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"}

# ì „ë¬¸ê°€ 4: ì„¤ë¬¸ ìš”ì•½ ì „ë¬¸ê°€
@tool
def summarize_survey_profile_tool(survey_profile: dict) -> str:
    """
    'analyze_survey_tool'ë¡œë¶€í„° ë°›ì€ ì •ëŸ‰ì ì¸ ì‚¬ìš©ì í”„ë¡œí•„(ë”•ì…”ë„ˆë¦¬)ì„ ì…ë ¥ë°›ì•„,
    ì‚¬ëŒì´ ì´í•´í•˜ê¸° ì‰¬ìš´ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ì˜ í…ìŠ¤íŠ¸ ìš”ì•½ ë³´ê³ ì„œë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    logging.info("--- âœï¸ 'ì„¤ë¬¸ ìš”ì•½ ì „ë¬¸ê°€'ê°€ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤. ---")
    try:
        summarizer_prompt = ChatPromptTemplate.from_template("ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì„±í–¥ ë¶„ì„ ë°ì´í„°ë¥¼ í•´ì„í•˜ì—¬, í•µì‹¬ì ì¸ íŠ¹ì§•ì„ ìš”ì•½í•˜ëŠ” í”„ë¡œíŒŒì¼ëŸ¬ì…ë‹ˆë‹¤. ì•„ë˜ <ì‚¬ìš©ì í”„ë¡œí•„ ë°ì´í„°>ë¥¼ ë³´ê³ , ì´ ì‚¬ëŒì˜ ì„±í–¥ì„ í•œë‘ ë¬¸ë‹¨ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.\n<ì‚¬ìš©ì í”„ë¡œí•„ ë°ì´í„°>\n{profile}\n[ë°ì´í„° í•­ëª© ì„¤ëª…] - FSC: í˜„ì‹¤ì ì¸ ì œì•½ ì¡°ê±´ (0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì œì•½ì´ í¼), PSSR: ì‹¬ë¦¬ì  ìƒíƒœ (0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì•ˆì •ì ), MP: í™œë™ ë™ê¸°, DLS: ì„ í˜¸í•˜ëŠ” ì‚¬íšŒì„±\n[ìš”ì•½ ì˜ˆì‹œ] 'ì´ ì‚¬ìš©ìëŠ” í˜„ì¬ ì‹œê°„ê³¼ ì˜ˆì‚°, ì—ë„ˆì§€ ë“± í˜„ì‹¤ì ì¸ ì œì•½ì´ í¬ë©°, ì‚¬íšŒì  ë¶ˆì•ˆê°ì´ ë†’ì•„ í˜¼ìë§Œì˜ í™œë™ì„ í†µí•´ íšŒë³µê³¼ ì•ˆì •ì„ ì–»ê³  ì‹¶ì–´í•˜ëŠ” ì„±í–¥ì´ ê°•í•˜ê²Œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.' ì™€ ê°™ì´ ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.")
        summarizer_chain = summarizer_prompt | llm | StrOutputParser()
        summary = summarizer_chain.invoke({"profile": survey_profile})
        logging.info("--- âœ… ì„¤ë¬¸ ìš”ì•½ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ---")
        return summary
    except Exception as e:
        logging.error(f"ì„¤ë¬¸ ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        return f"ì˜¤ë¥˜: ì„¤ë¬¸ ìš”ì•½ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"


# --- 7. ê°ë…ê´€(Supervisor) ì—ì´ì „íŠ¸ ìƒì„± ---
tools = [
    run_meeting_matching_agent_tool,
    analyze_photo_tool,
    analyze_survey_tool,
    summarize_survey_profile_tool,
]

final_supervisor_prompt_v3_1 = """
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ë‹¤ì–‘í•œ ìš”ì²­ì„ ì´í•´í•˜ê³ , ì—¬ëŸ¬ AI ì „ë¬¸ê°€ë“¤ì„ ì§€íœ˜í•˜ì—¬ ìµœì ì˜ ë‹µë³€ì„ ë§Œë“¤ì–´ë‚´ëŠ” ì´ê´„ ê°ë…ê´€ì…ë‹ˆë‹¤.

[ë‹¹ì‹ ì´ ì§€íœ˜í•  ìˆ˜ ìˆëŠ” ì „ë¬¸ê°€ë“¤]
- `run_meeting_matching_agent_tool`: ì‚¬ìš©ìê°€ ë§Œë“¤ë ¤ëŠ” ëª¨ì„ì˜ ìƒì„¸ ì •ë³´(ë”•ì…”ë„ˆë¦¬)ë¥¼ ë°›ì•„ Self-RAG ë°©ì‹ìœ¼ë¡œ ìœ ì‚¬ ëª¨ì„ì„ ì°¾ì•„ ì¶”ì²œí•©ë‹ˆë‹¤.
- `analyze_photo_tool`: ì‚¬ì§„ì„ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ìì˜ ì™¸ë©´ì  ì„±í–¥ì„ íŒŒì•…í•©ë‹ˆë‹¤.
- `analyze_survey_tool`: ì„¤ë¬¸ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ìì˜ ë‚´ë©´ì  ì„±í–¥ì„ ì •ëŸ‰ ë°ì´í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
- `summarize_survey_profile_tool`: ì„¤ë¬¸ ë¶„ì„ ë°ì´í„°ë¥¼ ì‚¬ëŒì´ ì´í•´í•˜ê¸° ì‰¬ìš´ í…ìŠ¤íŠ¸ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.

[ì‘ì—… ì§€ì¹¨]
1.  ì‚¬ìš©ìì˜ ìš”ì²­(`Human Message`)ì„ ë³´ê³ , ì–´ë–¤ ì¢…ë¥˜ì˜ ì‘ì—…ì¸ì§€ ëª…í™•íˆ íŒŒì•…í•˜ì„¸ìš”. ('ìœ ì‚¬ ëª¨ì„ ì¶”ì²œ' ë˜ëŠ” 'ìƒˆë¡œìš´ ì·¨ë¯¸ ì¶”ì²œ')
2.  ì‘ì—…ì— í•„ìš”í•œ ì „ë¬¸ê°€ë“¤ì„ í˜¸ì¶œí•˜ì„¸ìš”.
    - **('ìƒˆë¡œìš´ ì·¨ë¯¸ ì¶”ì²œ'ì˜ ê²½ìš°)** `analyze_survey_tool` -> `summarize_survey_profile_tool` -> `analyze_photo_tool` ìˆœì„œë¡œ í˜¸ì¶œí•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤. ë§Œì•½ ì‚¬ì§„ì´ ì œê³µë˜ì§€ ì•Šìœ¼ë©´, ì‚¬ì§„ ë¶„ì„ ë‹¨ê³„ë¥¼ ê±´ë„ˆë›°ê³  ì„¤ë¬¸ ë¶„ì„ ê²°ê³¼ë§Œìœ¼ë¡œ ì¶”ì²œì„ ìƒì„±í•˜ì„¸ìš”.
    - **('ìœ ì‚¬ ëª¨ì„ ì¶”ì²œ'ì˜ ê²½ìš°)** ì‚¬ìš©ìì˜ ì…ë ¥ ë°ì´í„°(ë”•ì…”ë„ˆë¦¬ í˜•íƒœ)ë¥¼ `run_meeting_matching_agent_tool` ì „ë¬¸ê°€ì˜ `meeting_info` ì¸ìë¡œ ê·¸ëŒ€ë¡œ ì „ë‹¬í•˜ì—¬ í˜¸ì¶œí•˜ê³ , ê·¸ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ë©´ ë©ë‹ˆë‹¤.
3.  [ë§¤ìš° ì¤‘ìš”] ë§Œì•½ 'ì„¤ë¬¸ ìš”ì•½'ê³¼ 'ì‚¬ì§„ ë¶„ì„'ì˜ ê²°ê³¼ê°€ ì„œë¡œ ìƒë°˜ë  ê²½ìš°, ì´ ì°¨ì´ì ì„ ëª…í™•íˆ ì¸ì§€í•˜ê³  ì–¸ê¸‰í•˜ë©°, ë‘ ê°€ì§€ ì„±í–¥ì„ ëª¨ë‘ ì•„ìš°ë¥¼ ìˆ˜ ìˆëŠ” ê· í˜• ì¡íŒ ìµœì¢… ì¶”ì²œì„ í•˜ëŠ” ê²ƒì´ ë‹¹ì‹ ì˜ ê°€ì¥ ì¤‘ìš”í•œ ì„ë¬´ì…ë‹ˆë‹¤.
4.  ëª¨ë“  ì „ë¬¸ê°€ì˜ ë³´ê³ ë¥¼ ì¢…í•©í•˜ì—¬, ì‚¬ìš©ìì—ê²Œ ì „ë‹¬í•  ìµœì¢… ë‹µë³€ì„ ì™„ì„±í•˜ì„¸ìš”.
"""

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", final_supervisor_prompt_v3_1),
        MessagesPlaceholder(variable_name="messages"),
    ]
)

supervisor_agent = create_react_agent(llm, tools, prompt=prompt)


# --- 8. API ì—”ë“œí¬ì¸íŠ¸ ì •ì˜ ---
class AgentInvokeRequest(BaseModel):
    messages: list

@app.post("/agent/invoke")
async def invoke_agent(request: AgentInvokeRequest):
    """
    ì‚¬ìš©ìì˜ ëª¨ë“  ìš”ì²­ì„ ë°›ì•„ ê°ë…ê´€ AI ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•˜ê³  ìµœì¢… ë‹µë³€ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        # ë³µì¡í•œ ë”•ì…”ë„ˆë¦¬ ë°ì´í„°ë¥¼ json.dumps()ë¥¼ ì‚¬ìš©í•´ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        processed_messages = []
        for role, content in request.messages:
            if isinstance(content, dict):
                # contentê°€ ë”•ì…”ë„ˆë¦¬ì´ë©´, JSON ë¬¸ìì—´ë¡œ ë³€í™˜
                processed_messages.append((role, json.dumps(content, ensure_ascii=False)))
            else:
                # ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹ˆë©´(ì¼ë°˜ í…ìŠ¤íŠ¸ ë“±) ê·¸ëŒ€ë¡œ ì‚¬ìš©
                processed_messages.append((role, content))

        input_data = {"messages": processed_messages}

        final_answer = ""
        for event in supervisor_agent.stream(input_data, {"recursion_limit": 15}):
            if "messages" in event:
                last_message = event["messages"][-1]
                if isinstance(last_message.content, str) and not last_message.tool_calls:
                    final_answer = last_message.content
        
        return {"final_answer": final_answer}
    except Exception as e:
        logging.error(f"Agent ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"AI ì—ì´ì „íŠ¸ ì²˜ë¦¬ ì¤‘ ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

# --- 9. Pinecone DB ì—…ë°ì´íŠ¸/ì‚­ì œ ì—”ë“œí¬ì¸íŠ¸ ---
class NewMeeting(BaseModel):
    meeting_id: str
    title: str
    description: str
    time: str
    location: str

@app.post("/meetings/add")
async def add_meeting_to_pinecone(meeting: NewMeeting):
    try:
        meeting_index_name = os.getenv("PINECONE_INDEX_NAME_MEETING")
        if not meeting_index_name: raise ValueError("'.env' íŒŒì¼ì— PINECONE_INDEX_NAME_MEETINGì´(ê°€) ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        embedding_function = OpenAIEmbeddings(model='text-embedding-3-large')
        vector_store = PineconeVectorStore.from_existing_index(index_name=meeting_index_name, embedding=embedding_function)
        
        full_text = f"ì œëª©: {meeting.title}\nì„¤ëª…: {meeting.description}\nì‹œê°„: {meeting.time}\nì¥ì†Œ: {meeting.location}"
        metadata = {"title": meeting.title, "description": meeting.description, "time": meeting.time, "location": meeting.location, "meeting_id": meeting.meeting_id}
        
        vector_store.add_texts(texts=[full_text], metadatas=[metadata], ids=[meeting.meeting_id])
        
        logging.info(f"--- Pineconeì— ëª¨ì„ ì¶”ê°€ ì„±ê³µ (ID: {meeting.meeting_id}) ---")
        return {"status": "success", "message": f"ëª¨ì„(ID: {meeting.meeting_id})ì´ ì„±ê³µì ìœ¼ë¡œ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤."}
    except Exception as e:
        logging.error(f"Pinecone ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Pineconeì— ëª¨ì„ì„ ì¶”ê°€í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@app.delete("/meetings/delete/{meeting_id}")
async def delete_meeting_from_pinecone(meeting_id: str):
    try:
        logging.info(f"--- Pineconeì—ì„œ ëª¨ì„ ì‚­ì œ ì‹œì‘ (ID: {meeting_id}) ---")
        meeting_index_name = os.getenv("PINECONE_INDEX_NAME_MEETING")
        if not meeting_index_name: raise ValueError("'.env' íŒŒì¼ì— PINECONE_INDEX_NAME_MEETINGì´(ê°€) ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        embedding_function = OpenAIEmbeddings(model='text-embedding-3-large')
        vector_store = PineconeVectorStore.from_existing_index(index_name=meeting_index_name, embedding=embedding_function)
        
        vector_store.delete(ids=[meeting_id])
        
        logging.info(f"--- Pineconeì—ì„œ ëª¨ì„ ì‚­ì œ ì„±ê³µ (ID: {meeting_id}) ---")
        return {"status": "success", "message": f"ëª¨ì„(ID: {meeting_id})ì´ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."}
    except Exception as e:
        logging.error(f"Pinecone ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Pineconeì—ì„œ ëª¨ì„ì„ ì‚­ì œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")